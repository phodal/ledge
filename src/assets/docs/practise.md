# 框架

## DevOps 框架

### CALMS

- **Culture – 文化：**公司各个角色一起担当业务变化，实现有效协作和沟通；
- **Automation – 自动化：**在价值链中尽量除去手工步骤；
- **Lean – 精益：**运用精益原则更频繁地交付价值；
- **Metrics – 度量：**度量并使用数据来优化交付周期；
- **Sharing – 分享：**分享成功和失败的经验来相互学习。

来自《[实例化 DevOps 原则](https://insights.thoughtworks.cn/instantiate-the-principles-of-devops/)》

### 三步法

- **The First Way：** System Thinking （系统思考：强调全局优化，避免局部优化）；
- **The Second Way：** Amplify Feedback Loops （经过放大的反馈回路：创建从开发过程下游至上游的反馈环）；
- **The Third Way：** Culture of Continual Experimentation And Learning（持续做试验和学习的文化：持续做试验，承担风险、从失败中学习；通过反复实践来达到精通）。

《凤凰项目》中的三步工作法：

![三步工作法](/assets/images/devops-three-ways.jpg '三步工作法')

扩展资料：《[The Three Ways: The Principles Underpinning DevOps](https://itrevolution.com/the-three-ways-principles-underpinning-devops/)》

#### 三步法实例化

《DevOps 实践指南》 对应的示例：

```mindmap
 - 三步工作法
   - 流动原则
     - 使工作可见
       - Kanban
         - 电子看板
         - 物理看板
         - 技术债看板
     - 限制在制制品数
     - 减少批量大小
     - 减少交接次数
     - 持续识别和改善约束点
       - 识别系统的约束点
          - 环境搭建
          - 代码部署
          - 测试的准备和执行
          - 紧耦合的架构
       - 决定如何利用这个系统约束点
       - 基于上述决定，考虑全局工作
       - 改善系统的约束点
       - 如果约束点已突破，回到第一点
     - 消除价值流中的困境和浪费
       - 类型
         - 半成品
         - 额外工序
         - 额外功能
         - 任务切换
         - 等待
         - 移动
         - 缺陷
         - 非标准或者手动操作
         - 填坑侠
   - 反馈原则
     - 在复杂系统中安全地工作
       - 管理复杂的工作，从中识别出设计和操作的问题
       - 群策群力解决问题，从而快速地构建新知识
       - 在整个组织中，将区域性的新知识应用到全局范围
       - 领导者要持续培养有以上才能的人
     - 及时发现问题
     - 群策群力，战胜问题获取新知
     - 在源头中保障质量
     - 为下游工作重心而优化
   - 持续学习和实验原则
     - 建立学习型组织和安全文化
     - 将日常工作的改进制度化
     - 将局部发布转化为全局优化
     - 在日常工作中注入弹性模式
     - 领导层强化学习文化
```

### 加速：高效 DevOps 的技术/管理实践

见：《[https://guides.sonatype.com/foundations/devops/principle-based-devops-frameworks/](https://guides.sonatype.com/foundations/devops/principle-based-devops-frameworks/)》

- 技术实践
  - 持续交付
    - 版本控制
    - 部署自动化
    - 持续集成（CI）
    - 基于主干的开发
    - 测试自动化
    - 测试数据管理
    - 安全前移（DevSecOps）
    - 持续交付（CD）
  - 架构
    - 低耦合架构
    - 赋权团队
  - 产品和流程
    - 客户反馈
    - 价值流
    - 以小批量方式工作
    - 团队实验
- 管理实践
  - 精益管理和监控
    - 轻量级变更批准流程
    - 监控
    - 限制在制品数
    - 可视化工作
  - 文化
    - 支持学习
    - 团队内协作
    - 工作满意度
    - 变革型领导

### DevOps 四大支柱

来源：（《Effective DevOps》）

- **协作**。协作是通过支持多人的交互和输入来构建一个特定结果的过程。
- **亲密性**。不仅需要发展和维护个人之间的协作关系，还需要在组织内部、整个行业的团队和部分之间也需要建立紧密的关系。
- **工具**。工具是一个加速器，可以基于当前的文化和方向推动变化。
- **规模化**。规模化强调的是组织在整个生命周期中，所采用的过程和关键点。

### 人、产品、流程和工具

来自《[实例化 DevOps 原则](https://insights.thoughtworks.cn/instantiate-the-principles-of-devops/)》

- 实例化 DevOps 原则
  - 人
    - 领导者身体力行持续改进 高于 关注工具和基础设施
    - 试验并改进流程 而非 指责人的过失
    - 产品思维 高于 项目思维
  - 产品
    - 质量和安全内建 而非 晚期度量和检查
    - 客户反馈 高于 按期交付
    - 基于不可变容器的微服务 高于 单体应用
  - 流程
    - 管理层实践 Improvement Kata 和 Coaching Kata 进行流程持续改进 高于 用结果导向进行管理
    - 全局优化 而非 局部优化
    - 单件流 高于 库存
  - 工具
    - 自动化 高于 手工
    - 基础设施及代码 高于 手工配置
    - 部署流水线 而非 每日构建

## DevOps 生命周期

```process-table
| 需求 | 开发  | 构建  | 测试 | 部署 | 运维 |
|-|-|-|-|-|-|
| 把运维人员作为首要干系人 | 小团队 | 构建工具 | 自动化测试 | 部署工具 | 监控 |
| 在开发需求时寻找他们的意见 | 有限的合作 | 支持持续集成 | 用户验收测试|支持持续部署|对错误情况做出响应 |
| | 单元测试 | | | |
```

效能提升过程：

```process-table
| 度量分析 | 实践固化 | 工程赋能 | 过程打通 |
|-|-|-|-|
```

# 价值驱动

一切从需求出发，再回归到需求

## 做正确的事

我们的诸多软件实践，只是在正确的做事情。取而代之的，我们要去做正确的事情，即**有价值**的。

### 看板原则

看板三原则：

- 可视化
- 限制在制品
- 管理流动

看板的六个实践：

- 可视化
- 限制在制品
- 管理流动
- 显式化流程规则
- 建立反馈环路
- 协作式改进、试验中演进

示例

```kanban
 - 开发看板
   - 待办
   - 分析
   - 待开发
   - 开发
   - 验收
   - 测试
   - 完成
```

应对变化

|          | 待办 | 分析 | 开发 | 测试 | 完成 |
| -------- | ---- | ---- | ---- | ---- | ---- |
|          |      |      |      |      |      |
|          |      |      |      |      |      |
| 紧急工作 |      |      |      |      |      |

### 消除浪费

软件开发的 7 种消费

- 部分完成的工作。
- 多余的特性。『没有比生产过剩更大的浪费』—— 大野耐一
- 重复学习。
- 工作交接。
- 延期。
- 任务切换。上下文切换
- 缺陷。

### 持续改进系统吞吐量

## 实例化需求

### 需求代码化

Jira、Trello 等以特定的数据结果存储在数据库中，那么必然可以从代码中回溯出来。

### 故事原则

Bill Wake 提出了一个好用户故事的验收标准——[INVEST 模型](http://xp123.com/xplor/xp0308/index.shtml)，它由六个单词的首字母组成，分别是

- Independent：每个用户故事应该是独立的，不会和其他用户故事产生耦合
- Negotiable：并不会非常明确的阐述功能，细节应带到开发阶段跟程序员、客户来共同商议
- Valuable：每一个用户故事的交付都要能够给用户带来用户价值
- Estimable：不需要能够准确的估计，但需要能辅助客户排定优先级
- Small：要小一点，但不是越小越好，要大小合适，可以更容易的圈定故事范围
- Testable：需要能够进行验收测试，最好能把 Test Case 提前加进去

### 用户故事

#### 用户故事示例

```gherkin
# language: zh-CN
场景大纲: 用户查看 “我的” 页面未读消息数量提示
  Given 我收到消息
  When 我在“我的”页面
  Then 我可以看到未读消息计数

Note

1. 后端记录用户设备 ID 和用户 ID，记录每种消息的最近查看时间，根据此时间后的消息数量计算未读消息数量；
2. 超过 99 未读显示省略图标；
3. 时间展示规则:
1). 一小时内，显示为 xx 分钟前
2). 小于 24 小时，显示为 xx 小时前
3). 间隔超过 24 小时且小于 48 小时，显示：昨天
4). 间隔超过 48 小时，显示：MM-DD
5). 间隔超过一周：YY-MM-DD （ eg: 2019-03-01）
```

#### 坏的验收标准

1. AC 太多：不超过 5 个
2. 用了很多 AND（没拆分好，偷懒）
3. 穷举每个测试用例
4. 定义实现细节
5. 用了太多只有部分人懂的“术语”
6. 很难理解，不易读

#### 好的验收标准

1. 用场景来拆分故事中的验收标准
2. 使用业务/用户/自然语言/领域特定语言
3. 以“我”出发来描述（同理用户）
4. 触发条件应该是个动作
5. 讲“故事”的方式来安排 AC 的顺序
6. 读起来如同验收测试，业务和用户都看得懂

如何检验验收标准（AC）是否写得足够好？

- 易懂：做到一看就懂了吗？
- 价值：核心用户意图和价值体现出来了吗？
- 逻辑：AC 的拆分、顺序的逻辑是清晰的吗？
- 简洁：做到最简洁了吗？
- 抽象：做到不涉及实现细节吗？
- 验收：能直接作为验收测试用例吗？

## 用户故事地图

> 故事地图是一门在需求拆分过程中保持全景图的技术。

### 故事地图步骤

- 准备工作：通过当前⽤户旅程挖掘用户需求，考虑解决方案
- 1.  基于解决⽅案设计未来⽤户旅程
- 2.  从核心场景出发，头脑⻛暴⽀撑每个⽤户任务的功能需求
- 3.  其他次要场景，头脑⻛暴⽀撑每个⽤户任务的功能需求
- 4.  头脑风暴支撑每个功能需求的支撑性需求

# 转型

## 人

来源：Scott Prugh 《[Continuous Delivery](https://www.scaledagileframework.com/guidance-continuous-delivery/)》

```process-step
 - I 型（专家）
   - 精通某个领域
   - 其它领域的技能或经验很少
   - 很快遇到瓶颈
   - 对下游的浪费和影响不敏感
   - 抵制灵活或者可变的计划
 - T 型（通才）
   - 精通某个领域
   - 拥有很多领域的技能
   - 能突破瓶颈
   - 帮助制订灵活和可变的计划
 - E 型
   - 精通某几个领域
   - 有多个领域的实践经验，执行能力强，能持续创新
   - 潜力无限
```

## 团队变革

## 组织变革

### 组织类型

#### 组织类型学模型

Ron Westrum 组织类型学模型：组织如何处理信息

| 病态型         | 官僚型               | 生机型         |
| -------------- | -------------------- | -------------- |
| 隐瞒信息       | 忽略信息             | 各级探索信息   |
| 消灭信使       | 不重视信使           | 训练信使       |
| 逃避责任       | 各自担责             | 共担责任       |
| 阻碍团队的互动 | 容忍团队的互动       | 鼓励团队间结晶 |
| 隐瞒事故       | 组织是公道和宽容的   | 调查事故根因   |
| 压制新想法     | 认为新想法会造成麻烦 | 接纳新想法     |

—— 《DevOps 实践指南》

### 打造学习文化

在 ThoughtWorks 拥有这么一系列的培训（《[在 ThoughtWorks 我们如何做内部培训？](https://insights.thoughtworks.cn/how-we-do-training-in-thoughtworks/)》

- ThoughtWorks 内部培训
  - ThoughtWorks University，面向毕业生
  - ThoughtWorks Immersion，面向社招
  - Session（分享，一个小时以内）
  - WorkShop（动手），如 OO BootCamp、Ruby BootCamp
  - ThoughtWorks 内训（原郑大晔校）
  - 组内培训
  - 社区

### 康威定律

# 闭环：加快系统流动

> 系统化思维

精益思想五大原则:

- 价值（Value）
- 价值流（Value stream）
- 流动（Flow）
- 拉动（Pull）
- 尽善尽美（Perfection）

## 1. 持续集成

如《Jenkins 权威指南》一书指出，持续集成需要几个不同的阶段

- 阶段一：无构建服务器
- 阶段二：夜间构建
- 阶段三：夜间构建 + 自动化测试
- 阶段四：加入度量指标
- 阶段五：更认真地对待测试
- 阶段六：自动化验收测试和自动化部署
- 阶段七：持续部署

### 自动化构建

好的构建工具应该支持以下的特性

- 依赖管理
- 增量编译
- 正确处理多个模块、应用之间的编译和资源管理任务
- 支持不同的环境配置
- 适应不断变化的产品需求
- 构建自动化

### 持续集成基本纪律

- 构建失败后，不要提交新的功能代码（仅限于修复）
- 提交前，在本地运行所有的提交测试
- 等持续集成测试通过后，再继续工作
- 回家之前，构建必须处于成功状态（CI 红不过夜）
- 时刻准备着回滚到前一个版本（CI Master）
- 在回滚之前，要规定一个修复时间
- 为自己导致的问题负责

### 通知

形式：

- 电子邮件
- RSS 订阅
- 即时消息
- IRC 通知
- 桌面通知
- 短信通知
- 移动通知
- 制造噪声
- 极端反馈设备（如 CI 灯）

### 分支策略

如何选择分支策略：

- 发布节奏。发布频率越高，应该离 master 越近。每天发布 -> 主干开发，每两周可以考虑暂存分支。
- 测试。前置 QA -> master，后置 QA -> release 上进行测试。
- 团队的规模。团队规模越大，离 master 越远；规模越小，基于主干。
- 工作流的适应成本。

#### 基于功能分支

#### Giflow

#### 主干开发策略

> 在版本控制的分支模型中，开发人员都在同一个被称为“主干“的分支中进行协作，多个开发人员的代码的汇聚在一起，从而避免创建长时间无法合并分支。因此，这种方式避免了合并地狱，也不会破坏构建，并从此让开发人员过上幸福的生活。

- 代码质量与知识共享
- 协同
- 部署频率和故障平均恢复时间

基本原则：

1. 所有修改都要尽可能出现在主干上。
2. 每个分支都应该在使用后删除。
3. 主干分支的提交都应该保证持续可用。

```mermaid
gitGraph:
options
{
    "nodeSpacing": 150,
    "nodeRadius": 10
}
end
commit
branch newbranch
checkout newbranch
commit
commit
checkout master
commit
commit
merge newbranch
```

## 2. 自动化

### 自动化演进路径

自动化演进路径（《SRE：Google 运维解密》）：

1. 没有自动化。手动将数据库主进程在多位个位置之间转移。
2. 外部维护的、系统特定的自动化系统。
3. 外部维护的、通用的自动化系统
4. 内部维护的、系统特定的自动化系统
5. 不需要任何的自动化系统

### 自动化部署

#### 平均部署时间

| 公司     | 部署频率    | 部署交付期 | 可靠性 | 客户响应 |
| -------- | ----------- | ---------- | ------ | -------- |
| Amazon   | 23000 次/天 | 分钟       | 高     | 高       |
| Google   | 5500 次/天  | 分钟       | 高     | 高       |
| Netflix  | 500 次/天   | 分钟       | 高     | 高       |
| Facebook | 1 次/天     | 小时       | 高     | 高       |
| Twitter  | 3 次/周     | 高         | 高     |
| 普通企业 | 9 个月/次   | 月或季度   | 低/中  | 低/中    |

—— 《凤凰项目》

### 自动化部署策略

来自《Java 持续交付》中对不同策略的特点及代码总结：

|                      | 单目标部署         | 一次性全部部署     | 最小服务部署                 | 滚动部署                     | 蓝/绿部署            | 金丝雀部署                   |
| -------------------- | ------------------ | ------------------ | ---------------------------- | ---------------------------- | -------------------- | ---------------------------- |
| 整体复杂度           | 低                 | 低                 | 中                           | 中                           | 中                   | 高                           |
| 服务中断             | 是                 | 是                 | 否                           | 否                           | 否                   | 否                           |
| 新旧版本混合         | 否                 | 否                 | 是                           | 是                           | 否                   | 是                           |
| 回滚过程             | 重新部署之前的版本 | 重新部署之前的版本 | 停止更新，重新部署之前的版本 | 停止更新，重新部署之前的版本 | 将流量切回原来的版本 | 停止金丝雀实例               |
| 部署中的基础设施支持 | 健康检查           | 健康检查           | 路由变更、健康检查           | 路由变更、健康检查           | 路由变更、健康检查   | 路由变更、加权路由、健康检查 |
| 监控要求             | 基础               | 基础               | 简单                         | 简单                         | 简单                 | 高级                         |

### 自动化测试

见测试

### 自动化运维

如（《SRE：Google 运维解密》）一书中介绍的『服务可靠度层级模型』：

```pyramid
 - 服务可靠度层级模型
   - 产品设计
   - 软件开发
   - 容量规划
   - 测试 + 发布
   - 事后总结问题根源分析
   - 应急事件处理
   - 监控
```

## 3. 代码化

### 版本化

每个 DevOps 阶段需要版本化的对象（《DevOps 最佳实践》）

| 阶段 | 版本化的对象                          | 工具化                         |
| ---- | ------------------------------------- | ------------------------------ |
| 规划 | 主题、史诗、特性                      | 需求管理工具                   |
| 编码 | S-CI、基线、数据模型                  | 软件管理工具 <br> 数据建模工具 |
| 构建 | 构建 ID                               | 构建服务器工具                 |
| 测试 | 测试数据 、测试用例、测试集、测试运行 | 测试管理 <br> 测试自动化工具   |
| 发布 | 发布说明、部署脚本、配置文件          | 发布工具                       |
| 运维 | 数据文件、协议                        | 数据文件管理工具               |
| 监控 | 监控配置文件                          | 监控系统                       |

### 基础设施即代码

> 基础设施即代码是一种基于从软件开发实践的基础设施自动化的方法。它强调系统及其配置的日常置备和变更具有一致性和可重复性。 —— 《基础设施即代码：云服务器管理》

#### 原则

- 系统能够被复制
- 系统是用完可扔的
- 系统是一致的
- 过程是可重复的
- 设计经常变更

#### 实践

1. 使用定义文件（definition file)
2. 自文档化的系统和流程
3. 一切版本化
   - 可追溯性
   - 回滚
   - 相关性
   - 可见性
   - 可执行性
4. 持续测试系统和流程
5. 小的变更，而不是指变更

### 流水线即代码

> 流水线即代码 (Pipeline as Code) 通过编码而非配置持续集成 / 持续交付 (CI/CD) 运行工具的方式定义部署流水线。

1. 构建步骤脚本化
2. 添加脚本到持续集成
3. 集成自动化验证
4. 分支策略

Jenkinsfile 最佳实践（来源：《[Pipeline Best Practices](https://jenkins.io/doc/book/pipeline/pipeline-best-practices/)》

1.  确保 Groovy 代码在流水线中只作为胶水。
2.  避免流水线中的 Groovy 代码过于复杂
3.  减少重复相似流水线的步骤
4.  避免调用 `Jenkins.getInstance`

使用共享库：

1.  不要覆写内建的流水线步骤
2.  避免巨大的全局变量声明文件
3.  避免非常大的共享库

示例（来源《[流水线即代码](https://insights.thoughtworks.cn/pipeline-as-code/)》：

```groovy
node('master') {
   stage('Checkout') {…}
   stage('Code Analysis') {…}
   stage('Unit Test') {…}
   stage('Packing') {…}
   stage('Archive') {…}
   stage('DEV') {…}
}
stage('SIT') {
   timeout(time:4, unit:'HOURS') {
       input "Deploy to SIT?"
   }
   node('master') {…}
}
stage('Acceptance Test') {
   node('slave') {…}
}
```

### Deploy as Code

aka Deployment process as code

- Dockerfile

### 文档代码化

#### 文档代码化

> 文档代码化，将文档以类代码的领域特定语言的方式编写，并借鉴软件开发的方式（如源码管理、部署）进行管理。它可以借助于特定的工具进行编辑、预览、查看，又或者是通过专属的系统部署到服务器上。面向非技术人员的文档代码化的一种常见架构模式是：编辑-发布-开发分离』，

它具备以下一些特征（《[文档代码化](https://www.phodal.com/blog/document-as-code/)》）

- 使用标记语言编写内容。如 markdown
- 可通过版本控制系统进行版本控制。如 git
- 与编程一致的编程体验（除了内容写不了测试）
- 持续部署，即修改完内容可自动发布。
- 与特定的形式组织内容索引。如以知识库的形式来组织内容。
- 特定的文本格式。如架构决策记录、静态内容生成，以用于以提供更好的用户体验
- 可支持 REST API。以通过编辑器来修改内容
- 可以支持多种方式的输出。如网站标准 HTML，又或者是 Docx、Latex 等
- 支持编辑、校对工作流
- 支持搜索
- 多人协作

#### 如何设计一个文档代码化系统？

1. 使用格式化的文档
2. 借助静态站点生成技术来发布系统
3. 通过定制标记语言扩充能力
4. 面向非技术人员实现编辑器

常见的实践有：

- [ADR](https://www.phodal.com/blog/documenting-architecture-decisions/)（Architecture Decision Records，即架构决策记录）。是一个类似于亚历山大模式（即：设计模式）的短文本文件。（虽然决策本身不一定是模式，但它们分享着力量的特征平衡。）每个记录都描述了一组力量和一个响应这些力量的决策。请注意，决策是这里的核心部分，所以特定的力量可能出现在多个 ADR（架构决策记录） 中。

对应的系统实践：

- 《[【架构拾集】基于 Markdown 文档展示系统设计](https://www.phodal.com/blog/architecture-in-realworld-markdown-based-document-system-design/)》
- [Ledge](https://github.com/phodal/ledge/)

#### 架构文档化

```quadrant
 - 选择架构描述方法
   - 正式
     - 能不到这里就不到
   - 徒劳
     - 回避这里
   - 社区
     - 演进到这里
   - 部落
     - 从这里开始

config: {"left": "易于分享", "right": "难以分享", "bottom": "难以变更", "top": "易于变更"}
```

## 4. 实例化需求

> 实例化需求是一组方法，它以一种对开发开发团队有所帮助的方式（理想情况下表现为可执行的测试）描述计算机系统的功能和行为，让不懂技术的利益相关者也可以理解，即使客户的需求在不断变化，它也具有很好的可维护性，可以保持需求的相关性。 从而帮助团队交付正确的软件产品。

### 实例化需求过程

1. 从目标中获取范围
2. 从协作中制定需求说明
3. 举例说明
4. 提炼需求说明
5. 频繁验证
6. 演化出一个文档系统

### 实例化需求步骤

1. 澄清价值

- 描述背景
- 理清目标/问题

2. 识别操作及操作步骤

- 列用户操作
- 画出工作流

3. 定义业务规则

- 列举规则

### 验收测试驱动开发

来源：《[验收测试驱动开发介绍（ATDD）](https://www.jianshu.com/p/0389360ac58f)》

- 讨论澄清阶段
  - 全组参与的针对需求和方案的讨论
  - 大家产出对需求和方案共同的理解
  - 通过明确验收测试方式澄清我们的实现方案
  - 验收测试方式将被自动化
- 开发阶段
- 用明确具体的验收测试方式来指导开发工作
- 验收测试的自动化和特性的开发可以并行开展
- 全组成员对验收测试的自动化负责，而不仅仅是测试人员
- 最终，我们的产品实现能让所有的自动化测试通过
- 交付阶段
- 我们要保证之前迭代所有的自动化验收测试能在新交付上通过
- 给所有利益相关者演示我们的新特性
- 收集反馈，讨论改进

# 反馈：建立快速反馈

or 强化反馈环

## 1. 测试

### 测试驱动开发

> TDD 是测试驱动开发（Test-Driven Development）的英文简称，是敏捷开发中的一项核心实践和技术，也是一种设计方法论。TDD 的原理是在开发功能代码之前，先编写测试用例代码，通过测试代码确定编写什么产品代码。

```graphviz
digraph {
  rankdir=LR;
  "红" -> "绿"[label="写一个失败的测试"];
  "绿" -> "重构"[label="让代码可以工作"];
  "重构" -> "红"[dir="back" label="重构"];
}
```

#### TDD 三定律

- **定律一** 在编写不能通过的单元测试前，不可编写生产代码
- **定律二** 只可编写刚发无法通过的单元测试，不能编译也算不通过
- **定律三** 只可编写刚好足以通过当前失败测试的生产代码

#### 测试驱动循环

1. 每个特性都从一个验收测试开始
2. 分离测量进度的测试和捕捉回归的错误的测试
3. 从最简单的成功场景开始测试
4. 编写您愿意读的测试
5. 看着测试失败
6. 从输入开发到输出开发
7. 针对行为进行单元测试，而非针对方法
8. 聆听测试
9. 调整循环

—— 《测试驱动的面向对象软件开发》

#### 实例化需求

#### Tasking？

### 整洁的测试

#### FIRST 原则

如 Robert C. Martin 在《代码整洁之道》所说的那样，好的测试应该是：

- 快速（Fast），测试应该够快。
- 独立（Indendent），测试应该相互独立。
- 可重复（Repeatable），测试应当可在任何环境中通过。
- 自足验证（Self-Validating），测试应该有布尔值输出。
- 及时（Timely），测试应该及时编写。

要我说的话，它应该还有：

- **同一人编写**，测试应该由开发业务代码的编写。这样他/他们才知道自己代码写得烂。
- **边界**，测试直接不影响业务代码。这里指的主要是 private -> public 的行为，又或者是业务代码中包含测试代码，而非因为测试对原有代码重构。
- **有效命名**。测试信息应该体现在方法名上，表达某一个特定的需求。

测试代码应该遵循生产代码的质量标准。

## 2. 代码回顾

来源：《深入核心的敏捷开发》

```mindmap
 - 代码回顾（CodeReview)
   - 目的：学习 vs 挑错
   - 重点
     - 代码 vs 作者
     - 习惯 vs bug
     - 模式 vs 反模式
   - 注意
     - 整洁代码 vs 我的写法
     - 整洁代码 vs 重新设计
   - 形式
     - 随机摄取代码（当天编写的）
     - 每日一次，每次半小时以内，每次回顾 200~300 行代码
```

PS：时间视真实的团队而定，如果不能每天进行代码回顾，时间一般控制在 0.5 ~ 1 小时内。

### 寻找什么？

基于：[Java Code Review Checklist](https://dzone.com/articles/java-code-review-checklist) 和 《Java 持续交付》

#### 理解性问题

包含以下几点

- 使用解决方案或者问题域的名称
- 使用含义显而易见的名称
- 尽量减少类和成员的可访问性
- 尽量减少局部变量的作用域
- 在单个逻辑组件（包、服务或者服务）中，要遵循『不要重复自己（DRY）』的原则
- 用代码来解释代码
- 使用异常而不是难懂的错误代码，并且不返回 null

#### 语言规范问题

- 对可恢复条件使用已检查问题，对编程错误使用运行时异常
- 检查参数的有效性，使其尽可能接近规范要求或者用户输入
- 标明哪些参数可以为 null
- 在公共类中，尽量使用访问器方法，而非公共字段
- 通过接口来引用对象
- 使用枚举来替换 int 常量
- 使用标记接口（marker interface）来定义类型
- 同步对共享可变数据的访问
- 优先使用 executors 来执行任务和线程
- 将线程安全记录到文档中

#### 安全性问题

- 应该检查输入数据的大小和范围，始终保证提供给数据存储、中间件或第三方系统的任何输入数据都是安全的。
- 不要在日志中记录高度敏感的信息
- 不要在异常中保留敏感信息（如，文件路径、系统内部逻辑、配置信息等）
- 考虑在使用后从内存中清除高度敏感的数据。
- 遵循最小权限原则
- 将其他与案例相关的信息记录到文档中

#### 性能问题

- 注意低效的算法（如多层循环）
- 避免创建不必要的对象
- 注意字符串连接的性能损失
- 避免过度使用同步，并保持同步代码块范围尽可能小
- 注意算法中潜在的死锁或活锁
- 确保正确配置了线程池和缓存

### 基本实践

基本过程：

```process-table
| 回顾 | 概述 | 检视 | 总结 |
|-|-|-|-|
| 回顾上次提到的问题及解决情况 | 简述本次检视的代码内容 | 逐行代码进行检视，讨论和记录问题 | 复述发现的问题和结论，做好记录 |
```

实践：

- 寻找一个适合多人回顾的显示器
- 线下集中优先，远程次之
- 制定回顾时间，及对应的回顾组织者（定期更换）
- 先讲述用户故事（业务视角），再讲述实现视角
- 当天代码，当天回顾
- 未提交的代码不回顾（除非遇到问题）
- 并非所有的代码都要回顾（根据需要）
- 回顾时，随时记录修改的内容
- 下次回顾时，需要检视上次的修改部分
- 意见不统一时（小问题以规范为主），可以再次讨论（或是会议）
- 功能过大时，可以考虑以小型 session / 专题的形式讲述

回顾的方式：

- 由提交者，讲述编写的代码
- 由其他/她人讲述提交者编写的代码

不能按时回顾时：

- 改天，并拿出更多的时间
- 积累过多未回顾代码时，Tech Lead 应当检视其他/她人写的代码

### 代码审查自动化

#### PMD

#### Checkstyle

#### Findbugs

## 3. 小步前进

> 小步前进是一系列步骤的集合，其目的是：集成越早问题越小。

### 小步提交

1. 一次本地提交只做一件事情
2. 一次远程提交只做一个用户故事
3. 频繁进行本地提交

### Git Hooks

Git 钩子列表：

```bash
applypatch-msg     post-merge         pre-auto-gc        prepare-commit-msg
commit-msg         post-receive       pre-commit         push-to-checkout
post-applypatch    post-rewrite       pre-push           update
post-checkout      post-update        pre-rebase
post-commit        pre-applypatch     pre-receive
```

Commit Hook 示例：

```process-table
| 执行提交脚本 | 执行 pre-commit  | 执行 Checkstyle| 执行预置的 lint | 提交代码 |
|-|-|-|-|-|
| git-cz | husky | checkstyle | lint-staged | git commit |
| conventional-changelog| commitlint | prettier  | | |
| |  | | | |
```

Push Hook 示例：

```process-table
| git push | 执行 prePush | 执行 lint | 执行 testing | 提交 |
|-|-|-|-|-|
```

提交信息规范（《[如何好一个 Git 提交信息及几种不同的规范](https://www.phodal.com/blog/how-to-write-a-better-git-commit-message/)》）：

- **build**: 影响构建系统或外部依赖关系的更改（示例范围：gulp，broccoli，npm）
- **ci**: 更改我们的持续集成文件和脚本（示例范围：Travis，Circle，BrowserStack，SauceLabs）
- **docs**: 仅文档更改
- **feat**: 一个新功能
- **fix**: 修复错误
- **perf**: 改进性能的代码更改
- **refactor**: 代码更改，既不修复错误也不添加功能
- **style**: 不影响代码含义的变化（空白，格式化，缺少分号等）
- **test**: 添加缺失测试或更正现有测试

## 4. 高频率发布

## 5. 可视化

### 监控

### 监控分层模型

出自《DevOps 最佳实践》一书：

```mindmap
 - 监控分层模型
     - 系统监控
       - 服务监控
          - 基础设施服务
          - 应用程序服务
       - 资源监控
          - CPU
          - 内存
          - 带宽
          - ……
       - 内建监控
          - 应用中的监控机制
       - 事件监控
          - 基础设施组件事件
          - 基础设施服务事件
          - 应用程序事件
     - 应用程序监控
       - 应用程序接口监控
          - 健康度？
       - 基础设施服务监控
     - 信息系统监控
       - 信息系统 E2E 监控
       - 基础设施 E2E 监控
       - 基础设施域监控
     - 链路监控
       - 信息流 E2E 监控
       - 业务流程 E2E 监控
       - 终端用户 E2E 监控
```

### Dashborad

Dashing: http://dashing.io/

# 持续：持续创新文化

如 RedHat 官方在 [DevOps 文件](https://www.redhat.com/zh/topics/devops) 所讲述：

> DevOps 十分依赖重视开放性和透明度的协作文化。实施 DevOps 就等于应用开源原则和实践，因为 DevOps 的文化价值观与开源社区和敏捷性工作方法的价值观紧密交融。

or 持续实验文化

## 持续学习

### 技术打磨

#### 练习入门

出自《[演进：在工作的前三年里快速成长（练习篇）](https://www.phodal.com/blog/evolution-how-to-practise/)》

#### 练习的进阶

诀窍（《[练习技术的诀窍](https://www.phodal.com/blog/how-to-make-practise-works/)》

- 可持续性：寻求量变
  - 让自己保持稳定的作息
  - 稳定的输出
  - 有充分的练习内容
- 可度量
  - SMART
- 可视化：产生仪式感
- 小步前进
- 里程碑：成就感

建议:

- 从模仿开始
- 时间限制的练习
- 日常积累灵感
- 用专业的工具

#### 高产

> 精通 one，学习 another，关注 next。 —— justjavac

高产策略（《[如何高产](https://www.phodal.com/blog/how-to-be-high-performance/)》

- 制定时间、目标和产出策略
  - 长期目标，一个小目标
  - 时间投入收益比对
  - 知识沉淀：输出优于输入
  - 不造轮子，再造轮子
  - 技能图谱：寻找、练习、升华
- 练习所需要的技能
- 寻找合适的工具，并打磨工具和手艺
- 练习速度、质量
- 通过休息来提出产出

## 持续交付

> 持续交付是一组实践规范和指导原则，能够让软件交付团队在短时间内产生有价值的、稳定的软件。

持续部署的发布工程哲学：

- 自服务模型
- 追求速度
- 密闭性
- 强调策略和流程

### 核心的构建管道

——《Java 持续交付》

```process-step
 - 开发机器
   - 所有的测试可以在本地运行
   - 提交代码
 - Git 服务器
 - 持续集成服务器
   - 代码质量（Sonar）分析
   - 上传制品仓库（Nexus）
   - 单元测试
   - 集成测试
   - 容错测试
   - 功能测试/压力测试编排
   - 开发编码
 - QA 环境
   - 验收测试
   - 性能测试
   - 探索性测试（API 和 UI）
   - 验收和性能测试（Cucumber 和 JMeter）
   - 功能和压力测试
   - -> 中心化日志
 - 预发布环境
   - 冒烟测试
   - 压力测试
   - 指标监控
   - -> 中心化日志
 - 生产环境
   - 冒烟测试
   - 主动监控和报警
 - 反馈
   - 是否验证了原始的设想？
   - 如何改进业务架构？
   - 如何提升运维能力？
```

## 追求卓越

### 测试即文档

『单元测试是可执行的文档』

## 优化

### 童子军规则

童子军有一条规则：“永远保持离开时的露营地比你发现它时更整洁”。

> 提交的代码要比检出的更好

#### 避免破窗效应

> 破窗效应：及时矫正和补救正在发生的问题。

以一幢有少许破窗的建筑为例，如果那些窗不被修理好，可能将会有破坏者破坏更多的窗户。最终他们甚至会闯入建筑内，如果发现无人居住，也许就在那里定居或者纵火。又或想像一条人行道有些许纸屑，如果无人清理，不久后就会有更多垃圾，最终人们会视若理所当然地将垃圾顺手丢弃在地上。因此破窗理论强调着力打击轻微罪行有助减少更严重罪案，应该以“零容忍”的态度面对罪案。

此理论描述了社区失序的五个阶段：

1. 社区开始出现失序的情形，部分居民迁出社区。
2. 未能迁离社区的居民因担心自身安全，对区内的事务漠不关心。
3. 地区的监察力下降，社区的治安进一步恶化。
4. 区内更多的居民迁走，仍然留在区内的居民则更加退缩，减少外出时间。
5. 外来的犯罪份子入侵社区，令犯罪数字持续上升。

对应的软件开发失序：

1. 少部分开发人员不关注代码质量。如测试的编写
2. 开发人员关注于交付速度，不关心代码质量。
3. 项目代码质量进一度恶化
4. 改一个 bug，出现更多的 bug
5. 项目无法维护，只能重写

### 技术债务管理

技术债治理的四条原则（《[技术债治理的四条原则](https://insights.thoughtworks.cn/managing-technical-debt/)》）：

1. 核心领域优于其他子域
2. 可演进性优于可维护性
3. 明确清晰的责任定义优于松散无序的任务分配
4. 主动预防优于被动响应

# 闭环

## 回到故事的开始

## 新项目检查清单

对于一个新项目来说，我们可以从下述的四个维度来检查项目的准备情况：

- Process，关注于从权限管理、获取代码、部署上线、代码集成等的流程。
- People，连接利益相关者、第三方合作伙伴（组织外）、协作团队（组织内）、团队成员等相关的人。
- Tech，包含了技术远景、文档（文档即代码）、项目代码、技术栈、软件库管理等。
- Business，涵盖了业务远景、业务需求、跨功能需求等业务相关功能的需求。

详见：[New Project Checklist](https://phodal.github.io/new-project-checklist/)

## 构建可信的软件系统

来源：[构建可信软件系统](https://github.com/phodal/trusted)

1. 可信的软件安全策略
2. 高度自动化的工作流
3. 设计架构适应度
4. 完善工具与基础设施
5. 高效的测试策略
6. 更好的知识传递
7. 框架与模块的与时俱进
8. 边界限定的系统架构
9. 持续偿还的技术债务
10. 强有力的个人 & 愿意改进的团队

# 敏捷实践

## 极限编程实践

极限编程的价值观（《学习敏捷》）：

- **沟通**。每个团队成员都清楚其他人在做什么。
- **简化**。开发 保内尽量让写出的代码简单、直接。
- **反馈**。不断进行测试和反馈，以保证产品的持量。
- **勇气**。每个团队成员都应该专注于为项目作出更佳的选择，即使这意味着不得不抛弃失败的方案而从不同的角度去解决问题。
- **尊重**。每个团队成对项目都是重要的、有价值的。

对应的实践整理如下：

```process-step
 - 反馈
   - 测试驱动开发
   - 计划游戏
   - 用户代表
   - 结对编程
 - 持续流程
   - 持续集成
   - 代码重构
   - 小的发布
 - 代码理解
   - 简单设计
   - 代码集体所有制
   - 编码标准
   - 系统隐喻
 - 工作环境
   - 每周 40 小时
```

## 结对编程

模式：

- 领航员和驾驶员（Driver-Navigator）。键霸出没，请小心。
- 乒乓模型。基于测试驱动开发的模式
- 鼠标和键盘模式。

注意事项：

1.  多沟通
2.  确定开发任务列表
3.  定期交换小伙伴
4.  可持续的结对工作
5.  多给新人机会
6.  勇敢加勇敢
7.  反馈
8.  不是所有的场景都适合结对

## 持续集成

### 依赖管理

#### 二进制管理 vs 源码依赖管理

|          | 二进制依赖管理                                                       | 源码依赖管理                                                                          |
| -------- | -------------------------------------------------------------------- | ------------------------------------------------------------------------------------- |
| 构建速度 | 直接使用二进制文件，速度快                                           | 所有代码全量构建，速度慢                                                              |
| 重构支持 | 不能同时修改依赖的代码和项目自身的代码，重构困难                     | 可以同时修改项目自身和依赖的代码，重构简单                                            |
| 提交操作 | 不同的代码仓库在不同的工作空间下分别提交代码，管理简单，效率较高     | 在同一个工作空间下管理多个仓库代码，代码修改分别提交到不同的仓库，容易出错 ，效率较低 |
| 开发调度 | 调度时无法跟踪到源码，即使发现依赖的问题，也无法修改                 | 调试时可以看到源码，发布问题时也可以即时修改                                          |
| 响应变化 | 发布需求和问题时，需要寻找依赖提供方的帮助，并放在下一次的开发计划中 | 可以直接修改并马上应用                                                                |

# 需求管理

## 设计思维

来源：《[ ThoughtWorks 唐婉莹——从 1 到 100，设计思维提升产品体验 ](https://myslide.cn/slides/7741))

### 设计思维是什么？

如何进行创新探索的方法论，其中包含了很多创新的方式。

他的宗旨是

- 从用户出发：解决问题的出发点是用户需求，而不是商业或技术先行
- 两种思维结合： 分析性思维和直觉性思维相结合
- 发散与收敛： 善用发散－收敛的方法
- 协作方式：共同认可的团队协作式设计，角色与角色的重叠

```process-card
| 实践 | 设计 | | 思维 |  探索 |
|-|-|-|-|-|
| 场景、需求 | 用户、目标  | | 方案、产品 | 测试、优化 |
| 探寻改进问题 | 定义用户类型 | 定位价值主张  | 发散解决方案 | 原型验证 |
| 探寻关键用户 | 识别场景需求 | | 优先级排序  | 优化方案 |
| 深度用户访谈 | 寻找机会点   | | 原型设计  | |
```

# 测试

```quadrant
 - 测试四象限 （Brain Marick）
     - 自动/手动
       - 用户故事测试
       - 端到端测试
       - 回归测试
       - ……
     - 手动
       - 探索式测试
       - 用户验收测试
       - 用户演示（showcase）
       - 用户培训
       - 安全测试（业务部分）
       - ……
     - 自动化
       - 单元测试
       - 组件测试
       - 集成测试
       - ……
     - 工具
       - 安全测试（技术部分）
       - 性能测试
       - 辅助功能测试
       - ……

config: {"left": "支持团队", "right": "评价产品", "bottom": "面向技术", "top": "面向业务"}
```

## 敏捷测试原则：

—— 出处：《[ThoughtWorks 的敏捷测试](https://insights.thoughtworks.cn/thoughtworks-agile-testing/)》

1. 我们的目标在于和团队一起尽快地交付高质量软件。
2. 测试人员尽早参与软件早期阶段，与所有团队角色合作，通过实例化需求，确保对业务价值理解的一致性。
3. 测试人员关注生产环境状态，收集数据，指导和优化前期的分析、开发和测试。
4. 测试人员和开发人员同处一个产品项目团队，而不是独立的测试团队或部门。
5. 测试人员负责探索性测试，和开发人员结对，设计、实现和维护自动化测试。
6. 自动化测试在流水线中持续精准执行，快速发现每次代码提交对于已有功能的影响
7. 测试数据对于自动化测试是充分的，并能按需获得。
8. 测试活文档化，和代码一起，作为知识资产进行版本化管理。
9. 自动化测试需要有效的分层。
10. 预防缺陷，而不是关注缺陷的数量。

### 团队为质量负责

所有人应该对质量负责：

![淘宝内测线上 bug 示例](/assets/docs/images/ios-bug-taobao.jpg '淘宝内测 bug 示例')

从流程上来说：

- 缺乏单元测试。
- Code Review 没有有效实施。
- 没有结对编程。内部存在一定的结对编程
- 测试。测试缺乏对这种场景的测试情况
- 紧急修复。热修复不支持，只能通过新版本解决。

## 测试金字塔

```pyramid
 - 测试金字塔
   - 集成测试
   - 组件测试
   - 单元测试
```

- **单元测试**。
- **集成测试**。
- **系统测试**。

### 生产测试

又被称之为黑盒测试，包含了：

- 黑盒测试
- 压力测试
- 金丝雀测试

## 测试策略

来源：《[一页纸测试策略](https://mp.weixin.qq.com/s/cEkI3Duyg-Jqk-TTFwKVqQ)》

> 质量内建，旨在软件交付过程的早期发现并预防缺陷，将任务移动到软件开发生命周期的左侧来提高质量，测试人员可以从需求分析阶段开始介入。

```process-table
| 需求分析 | 开发前 | 开发  | 开发完成 | 测试|  发布前|已发布 |
|--|--|--|--|--|--|--|
| 用户故事评审 | 特性启动 | 单元测试 | 用户故事验收 | 用户故事测试 | 回归测试 | 监控 |
| 估算 | 测试用例设计 | 组件测试 | 底层测试评审 | 探索式测试 | 发布指南 | 支持 |
| 方案设计 | 用户故事启动 | | 发布可测试版本 | 缺陷管理 | 用户验收测试 | 质量分析 |
| 迭代计划 | 测试计划 | | | 风险评估 | 发布版本确认 | |
| | | | | 集成测试 | | |
| | | | | 端到端测试 | | |
```

### 自从而下的测试

- 优先考虑用户体验
- 需要更强的开发技能
- 更多的测试覆盖率
- 实现和修复的时间长

### 自下而上的测试

- 平衡各类测试
- 更多的低级别测试

## 自动化测试

### 引入自动化测试的策略

- 先测新增代码
- 先测高价值代码
- 先测稳定代码
- 先做冒烟测试
- 为缺陷增加测试

### 自动化测试成本

#### 一次性投入

- 学习的成本
- 搭建环境的成本

#### 持续性投入

- 编写自动化测试用例的成本
- 维护自动化测试用例的成本

### 自动化测试原则

- 测试用例之间保持独立
- 一个测试用例只做一件事情
- 测试的结果必须稳定
- 单元测试用例必须足够的快

测试过慢

- E2E 测试定期执行
- 拆分测试策略
- 检视以往的测试。删除无用的
- 并行执行测试

### 三段式测试

- BDD 方式（Given-When-Then）
- 设置-操作-断言（Arrange-Act-Assert）

#### BDD

Given-When-Then 表达方式可以称之为一个公式一个模板，这种方式旨在指导程序员为“用户故事”编写测试用例变得方便。

- Given 一个上下文，指定测试预设。以 Given 开头的步骤，表示运作开始时系统所处的位置。
- When 进行一系列操作，即所要执行的操作。以 When 开头的步骤，表示用户在系统上所执行的操作。
- Then 得到一系列可观察的后果，即需要检测的断言。以 Then 开头的步骤，表示前面步骤的结果。

E2E 定义示例：

```javascript
defineSupportCode(function ({ Given, When, Then }) {
  Given('当我在网站的首页', function () {
    return this.driver.get('http://0.0.0.0:7272/');
  });

  When('输入用户名 {string}', function (text) {
    return this.driver.findElement(By.id('username_field')).sendKeys(text);
  });

  When('输入密码 {string}', function (text) {
    return this.driver.findElement(By.id('password_field')).sendKeys(text);
  });

  When('提交登录信息', function () {
    return this.driver.findElement(By.id('login_button')).click();
  });

  Then('页面应该返回 {string}', function (string) {
    this.driver.getTitle().then(function (title) {
      expect(title).to.equal(string);
    });
  });
});
```

#### 设置-操作-断言

JUnit 示例：

```java
@Test
public void shouldReturnItemNameInUpperCase() {
    // Given
    Item mockedItem = new Item("it1", "Item 1", "This is item 1", 2000, true);
    when(itemRepository.findById("it1")).thenReturn(mockedItem);

    // When
    String result = itemService.getItemNameUpperCase("it1");

    // Then
    verify(itemRepository, times(1)).findById("it1");
    assertThat(result, is("ITEM 1"));
}
```

### 测试替身

> 有时候对被测系统(SUT)进行测试是很困难的，因为它依赖于其他无法在测试环境中使用的组件。这有可能是因为这些组件不可用，它们不会返回测试所需要的结果，或者执行它们会有不良副作用。在其他情况下，我们的测试策略要求对被测系统的内部行为有更多控制或更多可见性。 如果在编写测试时无法使用（或选择不使用）实际的依赖组件(DOC)，可以用测试替身来代替。测试替身不需要和真正的依赖组件有完全一样的的行为方式；他只需要提供和真正的组件同样的 API 即可，这样被测系统就会以为它是真正的组件！ ——Gerard Meszaros

| 类型         | 解释                                                                                                                                     |
| ------------ | ---------------------------------------------------------------------------------------------------------------------------------------- |
| Dummy Object | 不包含实现的对象（Null 也是），在测试中需要传入，但是它没有被真正地使用，通常它们只是被用来填充参数列表。                                |
| Fake         | 有具体实现的，但是实现中做了些捷径，使它们不能应用与生产环境（举个典型的例子：内存数据库）                                               |
| Stub         | 状态验证。返回固定值的实现                                                                                                               |
| Spy          | 行为验证。类似于 Stub，但会记录被调用那些成员，以确定 SUT（System Under Test）与它的交互是否是正确的                                     |
| Mock         | 由 Mock 库动态创建的，能提供类似 Dummy、Stub、Spy 的功能。开发人员看不到 Mock object 的代码，但可以设置 Mock object 成员的行为及返回值。 |

Mock 和 Stub 就是常见的两种方式：

- Stub 是一种状态确认，它用简单的行为来替换复杂的行为
- Mock 是一种行为确认，它用于模拟其行为

示例见：《[测试替身](https://growth.phodal.com/#%E6%B5%8B%E8%AF%95%E6%9B%BF%E8%BA%AB)》

### 简单规则

测试自动化应该遵循的简单规则

| 规则                             | 理由                                                                                                                                           |
| -------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| 单一职责                         | 易于调试：业务规则变更时易于修改                                                                                                               |
| 不要重复自己                     | 能够只改一个地方                                                                                                                               |
| 使用领域特定语言                 | 使测试人员相关的沟通更加容易                                                                                                                   |
| 抽象测试                         | 使测试更加易读                                                                                                                                 |
| 设置和清除（setup & teardown     | 可以重复执行测试                                                                                                                               |
| 避免访问数据库（尽可能）         | 访问数据库会使测试变慢（注意，有些情况还是需要访问数据库的）                                                                                   |
| 测试必须一直保持绿色             | 保证测试的信心：活文档                                                                                                                         |
| 应用公共测试标准（包括命名约定） | 可以共享代码或者测试的所有权，对测试达成共识                                                                                                   |
| 将测试（什么）和（如何）执行分隔 | 把什么（和为什么）抽象出来可以让各层分别完善；为使人读懂规则说明（测试），你可以增加更多示例，或者在不会影响精力规则的前提下更改底层自动化实现 |

## 测试驱动开发

依据 J.Timothy King 所总结的《[测试先行的 12 个好处](http://sd.jtimothyking.com/2006/07/11/twelve-benefits-of-writing-unit-tests-first/)》：

- 测试可证明你的代码是可以解决问题的
- 一面写单元测试，一面写实现代码，这样感觉更有兴趣
- 单元测试也可以用于演示代码
- 会让你在写代码之前做好计划
- 它降低了 Bug 修复的成本
- 可以得到一个底层模块的回归测试工具
- 可以在不改变现有功能的基础上继续改进你的设计
- 可以用于展示开发的进度
- 它真实的为程序员消除了工作上的很多障碍
- 单元测试也可以让你更好的设计
- 单元测试比代码审查的效果还要好
- 它比直接写代码的效率更高

详见：《[测试驱动开发](https://growth.phodal.com/#%E6%B5%8B%E8%AF%95%E9%A9%B1%E5%8A%A8%E5%BC%80%E5%8F%91)》

## 测试代码坏味道

详见：《[测试代码的坏味道](https://www.phodal.com/blog/test-bad-smell/)》

> 测试代码坏味道，是指单元测试代码中的不良编程实践（例如，测试用例的组织方式，实现方式以及彼此之间的交互方式），它们表明测试源代码中潜在的设计问题。

常见的测试坏味道：

- 空的测试。测试是生成的，但是没有内容。
- 忽略的测试。即测试被 Ignore
- 没有断言的测试。为了测试覆盖率而出现的测试
- 多余的 Println。调试时留下的讯息。
- 多重断言。每个测试函数只应该测试一个概念。

然后，再来个 Examples。

## API 测试

### Mock 测试

#### Mock Server

相关的工具：

- Mountebank，是第一个开源、跨平台、多协议的服务虚拟化工具。
- WireMock，是一个灵活的 API 模拟工具，可进行快速，强大和全面的测试。
- Stubby4j，是一个 HTTP stub 服务器，用于测试应用程序与 Web 服务（REST 等）的交互以及用于轻松测试的外部系统 stub。
- VCR/Betamax，可以记录和重放应用程序的网络流量。
- Hoverfly，是一个轻量的 API 服务模拟工具（有时候也被称作服务虚拟化工具）。除了回放和记录，Hoverfly 还支持监视、合成、修改、差异等模式。

### 契约测试

前提条件：消费者和生产者**都同意**需要一个约束彼此交互的契约。当只有消费者愿意采用契约测试时，是很难进行契约测试的。

#### 消费者驱动的契约测试

> 消费者驱动契约测试（Consumer-Driven Contracts Testing）背后的理念是定义每个服务消费者与提供者之间的契约，然后根据该契约对消费者和提供者进行独立测试，以验证他们是否符合契约约定的事项。

- [Moco](https://github.com/dreamhead/moco) + [Moscow](https://github.com/macdao/moscow)
- [Spring Cloud Contract](https://spring.io/projects/spring-cloud-contract)
- [Pact](https://docs.pact.io)

Resources: [Qixi's presentations](https://github.com/macdao/presentations)

## 测试覆盖率

### 测试覆盖率曲线

```echarts
{
    "xAxis": {
        "type": "category"
    },
    "yAxis": {
        "type": "value"
    },
    "series": [{
        "data": [0, 1, 5, 10, 9, 14, 15, 30, 40, 45, 54, 60, 58, 65, 60, 70, 75, 78, 76, 80, 82, 83, 84, 85, 86, 87, 92, 95, 98, 97, 99, 98, 99, 98, 97, 99],
        "type": "line",
        "smooth": true
    }]
}
```

# 度量

## 评估与度量

### DevOps 四个关键指标

```mindmap
 - 提升交付效能
   - 部署频率
     - 批量的大小
     - 自动化程度
     - 测试效率
     - 部署效率
     - 构建效率
     - ……
   - 交付前置时间
     - 需求稳定性
     - 任务交接
     - 等待时间
     - 团队速度
     - 返工庇
     - ……
   - 平均恢复时间
      - 监测的效率
      - 系统可配置性
      - 系统可维护性
      - 日志可用性
      - 架构质量
      - ……
   - 变更失败率
      - 自动化程度
      - 架构质量
      - 基础设施即代码
      - 测试质量
      - 测试代码质量
      - 代码质量
        - 编码规范
        - 重复代码
        - 测试覆盖率
        - 扇入扇出度
        - 圈复杂度
        - ……

```

### 成熟度评估表格模型

| 产品/项目 | 维度     | 指标项       | 指标解读                                   | 单位 | 数据采集方式 | 值  |
| --------- | -------- | ------------ | ------------------------------------------ | ---- | ------------ | --- |
|           | 交付效率 | 交付前置时间 | 需求前置时间：从需求提出，到上线的时间周期 | 天   | 自动/手动    | 1.5 |

```
| 产品/项目 | 维度 | 指标项 | 指标解读 | 单位 | 数据采集方式| 值 |
|-|-|-|-|-|-|-|
| | 交付效率 | 交付前置时间 | 需求前置时间：从需求提出，到上线的时间周期 | 天| 自动/手动 | 1.5 |
```

### 度量维度

- 需求管理：平均需求交付时长
- 开发管理：单元测试覆盖率、平均修复问题时间、问题打回率、代码注释率、代码质量
- 构建管理：构建频率、平均构建时间、构建成功率
- 部署管理：部署频率、平均部署时间、部署成功率
- 进度管理：任务燃尽数

### 常见低效原因

如我司同事说：《[DevOps —— 实施 DevOps 应该考虑的若干问题](https://blog.csdn.net/gudufeiyang/article/details/104190250)》

- **技术债务**：长期以来，开发团队只注重新功能的开发，不注意重构和流程改进，不注重新技术、新方法的运用。
- **团队协作和流程**：传统的方式，开发团队和运维团队的工作是分离的。他们有不同考核和业务目标；工作过程中的协作方式基本依靠流程或电子流。
- **系统架构不合理**：耦合度高，导致开发过程中的问题和领域不能被合理的分割，部署和运维过程也可能会导致整个系统崩溃。
- **研发方法和流程不科学**：研发过程不够敏捷，或者敏捷过程中的重要实践并未正确的执行。
- **人员技能偏低**：人员技能不足，无法支撑项目的架构设计、疑难技术的攻坚，无法满足业务的运维要求。
- **团队文化**：没有形成良好的学习文化，在面对问题时指责、惩罚是重要手段，无法让所有成员坦诚的分析问题，无法从失败中学习技术和经验。

### 度量标准

#### 可用性度量

可用性度量公式：

> 网站可用性百分比 = （该期间的总秒数 - 系统宕机的秒数） / 该期间的总秒数

| 可用性 %             | 每年的故障时间 | 每月的故障时间 | 每周的故障时间 | 每天的故障时间 |
| -------------------- | -------------- | -------------- | -------------- | -------------- |
| 55.5555555% (9 个 5) | 162.33 天      | 13.53 天       | 74.92 小时     | 10.67 小时     |
| 90% (1 个 9)         | 36.53 天       | 73.05 小时     | 16.80 小时     | 2.40 小时      |
| 95% (1.5 个 9)       | 18.26 天       | 36.53 小时     | 8.40 小时      | 1.20 小时      |
| 97%                  | 10.96 天       | 21.92 小时     | 5.04 小时      | 43.20 分钟     |
| 98%                  | 7.31 天        | 14.61 小时     | 3.36 小时      | 28.80 分钟     |
| 99% (2 个 9)         | 3.65 天        | 7.31 小时      | 1.68 小时      | 14.40 分钟     |
| 99.5% (2.5 个 9)     | 1.83 天        | 3.65 小时      | 50.40 分钟     | 7.20 分钟      |
| 99.8%                | 17.53 小时     | 87.66 分钟     | 20.16 分钟     | 2.88 分钟      |
| 99.9% (3 个 9)       | 8.77 小时      | 43.83 分钟     | 10.08 分钟     | 1.44 分钟      |
| 99.95% (3.5 个 9)    | 4.38 小时      | 21.92 分钟     | 5.04 分钟      | 43.20 秒       |
| 99.99% (4 个 9)      | 52.60 分钟     | 4.38 分钟      | 1.01 分钟      | 8.64 秒        |
| 99.995% (4.5 个 9)   | 26.30 分钟     | 2.19 分钟      | 30.24 秒       | 4.32 秒        |
| 99.999% (5 个 9)     | 5.26 分钟      | 26.30 秒       | 6.05 秒        | 864.00 毫秒    |
| 99.9999% (6 个 9)    | 31.56 秒       | 2.63 秒        | 604.80 毫秒    | 86.40 毫秒     |
| 99.99999% (7 个 9)   | 3.16 秒        | 262.98 毫秒    | 60.48 毫秒     | 8.64 毫秒      |
| 99.999999% (8 个 9)  | 315.58 毫秒    | 26.30 毫秒     | 6.05 毫秒      | 864.00 微秒    |
| 99.9999999% (9 个 9) | 31.56 毫秒     | 2.63 毫秒      | 604.80 微秒    | 86.40 微秒     |

#### 自动化测试度量指标

- 自动化测试用例数。
- 自动化测试覆盖率。
- 自动化测试通过率。
- 自动化测试执行时长。
- 自动化测试执行频率。

#### 质量属性

Lionheart 项目的场景属性场景：

| 质量属性 | 场景                                                              | 优先级 |
| -------- | ----------------------------------------------------------------- | ------ |
| 可用性   | 招标数据库无响应时，系统应记录错误日志，并在 3 秒内用历史数据代替 | 高     |
| 可用性   | 用户搜索公开的招标文件，一年中 99% 的时间都能正常获取结果列表     | 高     |
| 可伸缩性 | 可以在计划维护窗口（7 小时）内添加新服务器                        | 低     |
| 性能     | 当系统处于每秒 2 次搜索的平均负载时，用户可在 5 秒内看搜索结果    | 高     |
| 可靠性   | 招标文件更新应该在变更后 24 小时内反映出来                        | 低     |
| 可用性   | 用户触发的更新（如收藏招标文件）应在 5 秒内反映在系统中           | 低     |
| 可用性   | 系统可以处理每秒 10 次搜索的峰值负载，平均响应时间下降不超过 10%  | 低     |
| 可伸缩性 | 预设数据增长率为每年 5%，系统应该能轻松应付                       | 低     |

—— 《架构师修炼之道》

# 监控

不同级别的栈的监控目的

| 监控目标           | 数据来源       |
| ------------------ | -------------- |
| 发现检测           | 应用和基础设施 |
| 性能降低检测       | 应用和基础设施 |
| 容量规划           | 应用和基础设施 |
| 针对业务的用户反应 | 应用           |
| 入侵检测           | 应用和基础设施 |

## APM

> APM 即应用性能管理，主要指对企业的关键业务应用进行监测、优化，提高企业应用的可靠性和质量，保证用户得到良好的服务，降低 IT 总拥有成本(TCO)。

### Apdex

> Apdex 全称是 Application Performance Index，是由 Apdex 联盟开放的用于评估应用性能的工业标准。Apdex 联盟起源于 2004 年，由 Peter Sevcik 发起。Apdex 标准从用户的角度出发，将对应用响应时间的表现，转为用户对于应用性能的可量化为范围为 0-1 的满意度评价。

Apdex 定义了应用响应时间的最优门槛为 **T**，另外根据应用响应时间结合 T 定义了三种不同的性能表现：

- **Satisfied（满意）**：应用响应时间低于或等于 T（T 由性能评估人员根据预期性能要求确定），比如 T 为 1.5s，则一个耗时 1s 的响应结果则可以认为是 satisfied 的。
- **Tolerating（可容忍）**：应用响应时间大于 T，但同时小于或等于 4T。假设应用设定的 T 值为 1s，则 4 \* 1 = 4 秒极为应用响应时间的容忍上限。
- **Frustrated（烦躁期）**：应用响应时间大于 4T。

| Apdex 值           | 颜色 | 说明                                                   |
| ------------------ | ---- | ------------------------------------------------------ |
| 0.75 ≤ Apdex ≤ 1   | 绿色 | 表示应用、实例或事务被调用时响应很快，用户体验较满意。 |
| 0.3 ≤ Apdex < 0.75 | 黄色 | 表示应用、实例或事务被调用时响应较慢，用户体验一般。   |
| 0 ≤ Apdex < 0.3    | 红色 | 表示应用、实例或事务被调用时响应极慢，用户体验较差。   |

### 开源 APM

- [PinPoint](https://github.com/naver/pinpoint) 韩国开源的一个功能完备的 APM 系统，支持 JVM 性能数据采集、服务 Trace、告警等功能。它具有应用程序无侵入的应用特性。
- [ZipKin](https://zipkin.io/) 是 Twitter 开源的 Trace 工具，通过 Java 程序中引入客户端，可隐式拦截 Http、Thrift 等形式服务调用。
- [SkyWalking](https://skywalking.apache.org/zh/) 是一个开源 APM 系统，为微服务架构和云原生架构系统设计。它通过探针自动收集所需的指标，并进行分布式追踪。
- [Prometheus](https://prometheus.io/) 是一个开源的系统监控和报警工具。
- [CAT](https://github.com/dianping/cat) 是基于 Java 开发的实时应用监控平台，为美团点评提供了全面的实时监控告警服务。
- [Hawkular](https://www.hawkular.org/) 一个功能完备的 APM 系统，应用程序中嵌入 Hawkular 客户端，主动将采集数据通过 Http 或者 Kafka 传递给 Hawkular。

## 异常管理

### 异常分类

- 规划
  - 针对于所有的主题（Themes 大故事）和史诗故事（Epics）
  - 确认错误路径
  - 复用鱼骨图（石川图）找到导致错误路径的根本原因
  - 完成健康模型
- 编码
  - 针对每个特性
  - 识别额外的错误路径，识别根本原因（鱼骨分析）和完成健康模型
  - 完成事态分类
  - 构建单元测试用例来检测将要写入的事态
  - 创建包含异常处理的源代码
  - 在源代码中添加需要补充的事态，并调整鱼骨分析、健康模型和异常文件
  - 编写脚本来改变监控您的预设
- 构建
  - 编译源代码到目标代码，同时为错误路径运行单元测试用例
  - 确保所有的事态能够被单元测试覆盖
- 测试
  - 为错误路径运行涵盖监控预设的系统测试和系统集成测试
  - 确保所有事态能够被系统测试和系统集成测试命中
- 发布
  - 确保健康模型、事态分类和异常文件覆盖率
- 部署
  - 部署目标码、更新监控规则的脚本来更改监控的预设
- 运维
  - 当提供的解决方案无效时，向开发人员反馈
- 监控
  - 当事态没有被正确检测到时，向开发人员反馈
  - 当没有预见到的事态发生时，请开发人员调整鱼骨分析、健康模型和异常文件

## 监控

### 告警系统

各类告警架构对比（来源：[https://www.ituring.com.cn/article/497377](https://www.ituring.com.cn/article/497377)）

| 方案                         | 简述                                                                                | 实时流式计划（单条） | 分布式 | 状态管理（中间数据等） | 延迟   | 语言支持                       | Hadoop 整合  | 执行方式  |
| ---------------------------- | ----------------------------------------------------------------------------------- | -------------------- | ------ | ---------------------- | ------ | ------------------------------ | ------------ | --------- |
| 现有方案（Akka + Cassandra） | 状态依赖于外部存储（Cassandra 不一定抗得住）                                        | 支持                 | 不支持 | 不支持，依赖外部存储   | 不好说 | Java                           | 没有整合     |           |
| 现有方案改造（Akka + Redis   | 状态依赖于外部存储（Redis）衍生出其它不可能的单点故障，而且开发工作量和难度大       | 支持                 | 支持   | 不支持，依赖外部存储   | 不好说 | Java                           | 没有整合     |           |
| Spark Streaming              | Spark Streaming 哪里都好，就是不支持真正意义上的流式计划（单条）                    | 不支持（小批量）     | 支持   | 有状态（RDD）          | 秒级   | Java, Scala, Python            | 整合得比较好 | Stage     |
| Storm                        | Storm 为流式计算而生，但是无法满足我们需要状态管理的场景，需要引入 外部存储。另外？ | 支持                 | 支持   | 无状态                 | 毫秒级 | Ruby, Python, Perl, JavaScript | 整合一级     |           |
| Flink                        | 流计算方面综合了上面两者的优点，且基于 pipeline 模式要优于 Spark 的 Stage 模式      | 支持                 | 支持   | 有状态，自己管理内容   | 毫秒级 | Java, Scala, Python            | 整合非常好   | Pipelined |

#### OneAPM 架构

CEP 1.0：

```graphviz
digraph {
  rankdir=LR;
  Events -> Kafka -> Flink -> Redis
  Flink -> Cassandra
}
```

CEP 2：

解析计算引擎 -> 处理队列引擎 -> 分布式管理平台 -> 操作接口

SQL DSL -> Antlr -> AST Tree -> Process Model -> Event Process Result

```graphviz
digraph {
  rankdir=LR;
  "REST API" -> Kafka;
  "Metric Stream" -> Kafka;
  Kafka -> Akka;
  Akka -> MySQL;
  Akka -> Cassandra;
  Akka -> Druid;
}
```

# 分析

## 大数据处理

```process-table
| 数据源 | 数据集成 | 数据存储 | 数据分析 | 数据可视化 |
|-|-|-|-|-|
| 结构化数据 | 数据收集 | 对象存储 | 机器学习 | 报表 |
| 非结化数据 | 数据清理 | 文件存储 | 深度学习 | 图表 |
|          | 数据清理 | 数据库   |        | 仪表盘 |
|          | 数据处理 |         |        |       |
```

### 大数据工具

```process-step
 - 数据源
    - CSV
    - TXT
    - 音频、视频
    - 图片
    - 数据库
 - 数据集成
    - Spark
    - Flink
    - NiFi
    - Kafka
    - RabitMQ
    - Pentaho
 - 数据存储
    - HDFS
    - MINIO
    - Neo4j
    - PostgreSQL
    - MongoDB
    - MariaDB
    - Hbase
 - 数据分析
    - Scikit Learn
    - Keras
    - Numpy + Pandas
    - Tensorflow
 - 数据可视化
    - Dashing
    - D3.js
    - Leaflet
    - Echarts
```

### 数据-价值金字塔

```pyramid
 - 数据-价值金字塔
  - 驱动，价值，影响 <br> 更改，变化，交付：行动、
  - 整理，推荐，理解，推断，学习：预测
  - 结构，链接，元数据，标签，探索，交互，分享：报表
  - 清洗，聚合，可视化，问题：图表
  - 收集，展示，披露单条数据记录：数据记录
```

——《敏捷数据科学》

- **数据记录**。整个栈中对原子数据记录的处理和展示
- **图表**。从数据记录中提出数据的不同属性进行汇总，并把它们绘制成图表
- **执行**。从数据中提取关系和趋势，让我们能创建互动式图表进行数据探索
- **预测**。使用金字塔结构进行推断、预测和推荐
- **先去**。指引用户行为去创造和获取价值

### 敏捷大数据处理

```process-table
| 事件日志 | 采集器 | 海量存储 | 批量处理 | 分布式文件仓库 | 应用服务器 | 浏览器 |
|-|-|-|-|-|-|-|
```

- 事件日志。每个事件记录了在某个时间、什么环境下发生了什么事情。
- 采集器。负责事件的聚合。
- 海量存储。是一个能支持多个进程并行访问的文件系统。
- 分布式文件仓库。是用文档格式存储数据的结点集群。
- Web 应用服务器。能以最小开销将数据转换为 JSON，以便于客户端可视化。
- 浏览器。

示例：

```process-table
| 事件日志 | 采集器 | 海量存储 | 批量处理 | 分布式文件仓库 | 应用服务器 | 浏览器 |
|-|-|-|-|-|-|-|
| | Python -> AVRO 格式 |  MongoDB / ElasticSearch  |  Pig / Wonderdog | MongoDB / ElasticSearch | Flask |> Boostrap + D3.js / Echarts / NVD3.js |
```

## 数据可视化

### 可视化流程

```process-table
| 获取 | 分析 | 过滤 | 挖掘 | 表述 | 修饰 | 交互 |
|-|-|-|-|-|-|-|
```

### 数据展示形式

参见：[https://visual.phodal.com/](https://visual.phodal.com/)

## 数据驱动设计

# 安全

## 安全模型

### 分析：STRIDE 模型

| 威胁                               | 定义             | 对应的安全属性 |
| ---------------------------------- | ---------------- | -------------- |
| Spoofing（伪装）                   | 冒充他人身份     | 认证           |
| Tampering（篡改）                  | 修改数据或代码   | 完整性         |
| Repudiation（抵赖）                | 否认做过的事     | 不可抵赖性     |
| Information Disclosure（信息泄露） | 机密信息泄露     | 机密性         |
| Denial of Service（拒绝服务）      | 拒绝服务         | 可用性         |
| Elevation of Privilege（提升权限） | 未经授权获得许可 | 授权           |

### 度量：DREAD 模型

来源：《[STRIDE 模型和 DREAD 模型](https://ixyzero.com/blog/archives/745.html)》

| 等级             | 高（3）                                        | 中（2）                                    | 低（1）                |
| ---------------- | ---------------------------------------------- | ------------------------------------------ | ---------------------- |
| Damage Potential | 获取完全验证权限，执行管理员操作，非法上传文件 | 泄露敏感信息                               | 泄露其他信息           |
| Reproducibility  | 攻击者可以随意再次攻击                         | 攻击者可以重复攻击，但有时间限制           | 攻击者很难重复攻击过程 |
| Exploitability   | 初学者短期能掌握攻击方法                       | 熟练的攻击者才能完成这次攻击               | 漏洞利用条件非常苛刻   |
| Affected users   | 所有用户，默认配置，关键用户                   | 部分用户，非默认配置                       | 极少数用户，匿名用户   |
| Discoverability  | 漏洞很显眼，攻击条件很容易获得                 | 在私有区域，部分人能看到，需要深入挖掘漏洞 | 发现漏洞极其困难       |

## 实践

### 软件安全保障

### 定义 SSA

策略一：

1. 评估软件和制订修复计划
2. 定义数据的各种安全风险并进行分类，优先修复最严重的风险
3. 执行全面的代码检查
4. 进行必要的更改
5. 测试修复并在生产环境中验证它们确实是有效的
6. 制定防御机制来保护应用程序的访问及其管理的数据
7. 衡量你所做的这些更改的有效性
8. 以适当的方式培训管理者、用户和开发人员，以确保良好的应用安全性

策略二：

1. 把安全性要求置于产品需求之上（新应用开发时）
2. 定义并与开发人员沟通写代码时必须遵守的安全编码规范
3. 开发人员在写新代码时必须遵守的安全编码规范
4. 完成应用开发之后，要执行全面的代码检查
5. 对整个应用程序进行渗透测试和脆弱性评估
6. 评估测试结果以找到安全和业务之间的平衡点。
7. 执行所有需要进行的安全性修复
8. 重复步骤（5）

### OWASP

#### Top 10 风险

来源：[https://owasp.org/www-project-top-ten/](https://owasp.org/www-project-top-ten/)

版本：2017

- 注入
- 失效的身份认证
- 敏感信息泄露
- XML 外部实体(XXE)
- 失效的访问控制
- 安全配置错误
- 跨站脚本(XSS)
- 不安全的反序列化
- 使用含有已知漏洞的组件
- 不足的日志记录和监控

# DevOps 平台

## DevOps 数字化转型

```pyramid
 - DevOps 数字化转型框架
   - 价值驱动 <br> 精益，敏捷：创新组织、
   - 问题管理，事件管理，安全 <br> 知识管理，容量管理，成本管理等：精细化管理过程
   - 持续交付，统一监控，度量分析 <br> 容器管理，DevOps 服务，DevOps 作业：演进式能力中台
   - 持续集成，看板方法，凤凰系统 <br> 测试驱动开发，微架构，多级监控：持续性能力建设
```

## 研发服务平台

### 看板

在通过研讨会（快速启动会议）来开发项目/产品待办事项并确定项目范围之后，通常会使用带有评估和团队速度度量的看板（Scrum 板），可以观察到

- 确定预计完成时间（发布计划）
- 了解范围的添加将如何影响发布计划
- 未来/额外工作的规模
- 了解故事/工作如何在开发过程中移动
- 清晰地看到并识别生产力的瓶颈

那么看板应该层现哪些内容：

- 要完成的故事列表/项目的一般积压
- 任务的一般优先级（排序…通常越靠前优先级越高）
- 项目状态的高层次视图（是否已提取、正在处理或已完成）

## 持续交付平台

能力：

- 持续构建
- 持续测试
- 持续部署
- 持续发布
- 持续反馈

### 流程

示例：

### DevOps 流水线的跨功能需求

来源 《DevOps 架构师行动指南》

| 跨功能需求 | 质量关注点                                                                     |
| ---------- | ------------------------------------------------------------------------------ |
| 可重复性   | 重复相同操作的可能性的程度                                                     |
| 性能       | 执行 DevOps 操作所需要的时间和资源                                             |
| 可靠性     | 在一定时间周期内，DevOps 流水线及其内部各个软件保持服务状态的程度              |
| 可恢复性   | 失败的 DevOps 操作可以恢复到期望状态的程度，而仅对它所操作的应用千万极小的影响 |
| 互操作性   | 在特定环境下，不同 DevOps 工具通过接口有效地交换信息的程度                     |
| 可测试性   | 通过测试，DevOps 运维软件能够很容易地展示其错误                                |
| 可修改性   | 修改 DevOps 软件、过程或者应用程序运维环境所需要的工作量                       |

实现这种质量的技术总结：

| 跨功能需求 | 实现这种质量的技术                                                                                                                   |
| ---------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| 可重复性   | 维护活动的踪迹；版本控制一切；使用配置管理数据库来维护参数；在需要的地方执行                                                         |
| 性能       | 测量以判断过程中的瓶颈；在它未使用时拆解环境；在云上执行尽可能多的操作，因为云上的资源在未使用时可以释放                             |
| 可靠性     | 识别不同服务的故障率；对高故障率的服务建立镜像；通过工具尽可能快地检测故障，这些工具的任务是监控组件以发现其执行时间处在正常范围以外 |
| 可恢复性   | 在脚本中内置异常处理；为监控服务提供信息；保证生成合成的诊断信息以支持更快地调度                                                     |
| 互操作性   | 选择那些具有稳定接口和灵活脚本能力的工具；保证流水线的不同阶段的数据模型是一致的                                                     |
| 可测试性   | 为专用工具使用单元和集成测试；协调测试用例与监控规则                                                                                 |
| 可修改性   | 基于对工具的预期变化来更新模块化脚本；将运维运作封装到小模块中，这些小模块之间是松耦合的                                             |

示例：

```kanban
 - Kanban
     - ToDo
       - Story 4
       - Story 2
       - Story 3
       - Story 5
     - Analysis
     - Ready for Dev
        - Story 1
     - In Dev
     - Ready for QA
     - In QA
     - Done
```

## DevOps 服务平台

通用能力：

- 分布式缓存服务
- 数据库服务
- 消息队列服务
- 消息通知服务
- 秘钥管理服务

### 协同服务平台

如 ChatOps

通用能力：

- 即时消息工具
- 操作机器人
- 工具集成服务

## 部署平台

### PAAS 平台

—— 《Java 持续交付》

一个部署平台应该提供以下功能：

- 一个用于托管应用程序的、用户可以访问的物理位置
- 编程语言运行环境（如 Java）
- 持久化存储，可以是块存储或者数据库
- 能够访问（或者）安装所需要的中间件，如 ESB 或者消息队列
- 自我修复或自愈的，即可以自动重启失败的应用程序
- 服务发现，一种用来查找应用程序和其他服务和第三方服务的机制
- 基本的安全控制，包含经过案例加固的机器镜像、受限的端口访问，以及访问控制面板时要求提供强身份验证
- 了解运维方面产生的成本

### 容器管理平台

容器平台组件：

- 容器技术
- 容器调度 / 编排器。该组件负责启动、停止和管理容器进程，如 Docker Swarm 或者 Kubernetes。
- 存储。
- 网络。
- 服务。
- 持续交付服务。

### 持续交付容器

### FAAS 平台

#### Serverless

> Serverless 架构是指大量依赖第三方服务（也叫做后端即服务，即“BaaS”）或暂存容器中运行的自定义代码（函数即服务，即“FaaS”）的应用程序，函数是无服务器架构中抽象语言运行时的最小单位。在这种架构中，我们并不看重运行一个函数需要多少 CPU 或 RAM 或任何其他资源，而是更看重运行函数所需的时间，我们也只为这些函数的运行时间付费。 ——《[Serverless 架构应用开发指南](https://serverless.ink/)》

## 统一监控平台

通用能力：

- 基础设施监控服务
- 系统及组件监控服务
- 应用级监控服务
- 服务级监控服务
- 用户侧监控服务

## 度量分析平台

通用能力：

- 日志分析服务
- 数据可视化服务
- 分布式跟踪服务
- 大数据分析服务

# 微架构

## 后端：微服务

### 微服务要素

#### 12 要素

[12-Factor App](https://12factor.net/zh_cn/) 为构建如下的 SaaS 应用提供了方法论：

- 使用**标准化**流程自动配置，从而使新的开发者花费最少的学习成本加入这个项目。
- 和操作系统之间尽可能的**划清界限**，在各个系统中提供**最大的可移植性**。
- 适合**部署**在现代的**云计算平台**，从而在服务器和系统管理方面节省资源。
- 将开发环境和生产环境的**差异降至最低**，并使用**持续交付**实施敏捷开发。
- 可以在工具、架构和开发流程不发生明显变化的前提下实现**扩展**。

1. **基准代码** 。一份基准代码，多份部署
2. **依赖** 。显式声明依赖关系
3. **配置** 。在环境中存储配置
4. **后端服务** 。把后端服务当作附加资源
5. **构建，发布，运行** 。严格分离构建和运行
6. **进程** 。以一个或多个无状态进程运行应用
7. **端口绑定** 。通过端口绑定提供服务
8. **并发** 。通过进程模型进行扩展
9. **易处理** 。快速启动和优雅终止可最大化健壮性
10. **开发环境与线上环境等价** 。尽可能的保持开发，预发布，线上环境相同
11. **日志** 。把日志当作事件流
12. **管理进程** 。后台管理任务当作一次性进程运行

### 领域驱动设计

## 前端：微前端

## APP：基座化

# 分布式团队

TBD

## 远程工具

### 工具选型

[How to do effective video calls](https://martinfowler.com/articles/effective-video-calls.html)

#### 工具示例

## 离岸交付

# 工具

## 选择工具

### BDD 工具选型

> Behavior Driven Development，行为驱动开发是一种敏捷软件开发的技术，它鼓励软件项目中的开发者、QA 和非技术人员或商业参与者之间的协作。

《[BDD 框架对比: Cucumber.js vs Robot Framework vs Gauge.js](https://github.com/phodal/bdd-frameworks-compare)》

考虑到情况有：

- 团队水平
- 语言因素。持续学习文化
- 文档丰富度

## 制造工具

```process-table
| 个人实践 | 记录流程 | 形成统一语言 | 抽象原则与模式 | 标准化流程（工具） |
|-|-|-|-|-|
```
